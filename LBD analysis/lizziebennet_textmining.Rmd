---
title: "Text Analysis of The Lizzie Bennet Diaries"
output: 
  md_document:
    variant: markdown_github
    fig_width: 7
    fig_height: 6
---

# Inspiration
Inspired by [Julia's Silge's](http://juliasilge.com/) recent talk on [Tidytext](http://tidytextmining.com/) as part of [NASA Datanauts](https://open.nasa.gov/explore/datanauts/), and her blog posts, I decided to try my hand at some text analysis. Julia's examples focus on the works of Jane Austen. As Jane Austen has been adapted so many time, I decided to "adapt" Julia's talk for the modern works of Austen's book - specifically the Lizzie Bennet Diaries.

![](http://www.pemberleydigital.com/wp-content/uploads/2012/04/LBD-FacebookCover-Emmy.png)
[Image source](http://www.pemberleydigital.com/)


# The Lizzie Bennet Diaries
The [Lizzie Bennet Diaries](http://www.pemberleydigital.com/the-lizzie-bennet-diaries/) is a modern adaptation of Jane Austen's Pride and Prejudice for YouTube. The story is told through a series of Vlogs by Lizzie Bennet as part of a school project. The series, created by Hank Green and Bernie Su, first aired on April 9, 2012, making this year the **5th Anniversary** of the series! Altogether, the series filmed more than 100 video episodes with over 9.5 hours of video making it the longest adaption of Pride and Prejudice to date. 

Along with the main LBD channel, there are also some supporting channels. These allow others characters to tell parts of the story that Lizzie doesn't take part in. For example, Lydia's Vlogs include the story on how she meets George Wickham and their budding relationship. While not required viewing, these extra videos help round out the experience.

Since the series ended, 2 books have come out from the creators and writers of the original videos: one that follows the videos but adds some more detail to Lizzie's life, and one that focuses on Lydia's story after the series ends. 

![](https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-9/10329014_10202570761053579_3130716185504613915_n.jpg?oh=d6dce9e54a81d5d7864d68dfa2f6269c&oe=596C631F)
The Secret Diary of Lizzie Bennet, signed by most of the cast and writers

In honor of LBD's 5th Anniversary, let's do some LBD text analysis! **Happy Anniversary LBD!**

![celebration+lizzie+bennet](http://media3.giphy.com/media/10MjSRjJxjc6XK/giphy-downsized.gif)  
<div style='font-size:50%'>([Source link](http://elizabethgillies.tumblr.com/post/44897897466))</div>  


# Analysis
## Gathering Data
The first part of this analysis is grabbing all the text from YouTube. To access the API, I use the [`tuber`](https://soodoku.github.io/tuber/) package by Gaurav Sood.  

```{r, echo=T, eval=FALSE}
library(tidyverse)
library(tuber)

yt_oauth(app_id, app_password)
```

```{r, include=FALSE}
library(tuber)
library(tidyverse)

yt_oauth("998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com", "MbOSt6cQhhFkwETXKur-L9rN")
```

The fist step was to find the channel id to access the LBD playlist. I do a quick search for `lizziebennet` to find some videos that I know are part of the series.
```{r}
search <- yt_search("lizziebennet")[1:5, ] 
search %>% select(title, channelId)
```

With the channel ID in hand, I can now access the channel's resources to find the playlist ID, which I will use to access all the videos in that playlist. `list_channel_resources` for `tuber` creates a list of channel attributes and buried in that list in the playlist ID.
```{r}
# Channel Information
a <- list_channel_resources(filter = c(channel_id="UCXfbQAimgtbk4RAUHtIAUww"), part="contentDetails")

# Uploaded playlists:
playlist_id <- a$items[[1]]$contentDetails$relatedPlaylists$uploads
playlist_id
```

The YouTube API atomically pages videos so the max you get per page is 50. I know I need more than that, so I created a function that I can use recursively to get all the videos. (This code is not a simple way of doing what I need it to, but it works. I would love any comments on how to clean it up.)

```{r, message=FALSE}
# Get videos on the playlist
vids <- get_playlist_items(filter= c(playlist_id=playlist_id), max_results = 50) 
  vid_ids <- map(vids$items, "contentDetails") %>%
  map("videoId")  %>%
  unlist()
vid_info <- tibble(ids = vid_ids, page = vids$nextPageToken)

getVideos <- function(vid_info){
  pageToken <- vid_info$page[length(vid_info$page)]
  vids <- get_playlist_items(filter= c(playlist_id=playlist_id), page_token = pageToken)
  vid_ids <- map(vids$items, "contentDetails") %>%
    map("videoId")  %>%
    unlist()
  nextPageToken = ifelse(!is.null(vids$nextPageToken), vids$nextPageToken, NA)
  vid_info_new <- tibble(ids = vid_ids, page = nextPageToken)
  return(vid_info %>% bind_rows(vid_info_new) )
}

vid_info <- getVideos(vid_info)
vid_info <- getVideos(vid_info)

# check that I have all 112 videos
nrow(vid_info)
```
```{r}
# pass NA as next page to get first page
nextPageToken <- NA
vid_info <-{}

# Loop over every available page
repeat {
  vids <- get_playlist_items(filter = c(playlist_id=playlist_id), page_token = nextPageToken)
  vid_ids <- map(vids$items, "contentDetails") %>%
      map("videoId")  %>%
      unlist()
    
  vid_info <- vid_info %>% bind_rows(tibble(ids = vid_ids))
    
  # get the token for the next page
  nextPageToken <- ifelse(!is.null(vids$nextPageToken), vids$nextPageToken, NA)
    
  # if no more pages then done
  if(is.na(nextPageToken)){
     break
  }

}
```


Now that I have a list of video IDs, I can use `get_captions` to access the text of the videos. I also use `xmlTreeParse` and `xmlToList` to covert the caption into into an easily accessible lines of text. I put the text, video ID, and video title in a tibble for use in tidydata.

```{r, message=FALSE}
library(XML)

getText <- function(id){
  x <- get_captions(id, lang = "en")
  title <- get_video_details(id)$title
  a <- xmlTreeParse(x)
  text <- a$doc$children$transcript
  text <- xmlToList(text, simplify = TRUE, addAttributes = FALSE) %>% 
    tibble() %>%
    mutate( id = id, title = title)
  return(text) 
}

vid_ids <- vid_info$ids[3:112]
text <- map_df(vid_ids, getText) %>% set_names(c("text", "vid_id", "title"))
```

I don't actually want to refer to each video by it's full title, so I do some data munching to get each episode's number (1-100). Notice, the 10 Q&A videos do not get a episode number assigned to them. For the sake of this analysis, I've decided to only work with the main 100 episodes.

```{r, message=FALSE}
titles <- text %>%
  distinct(title) %>%
  mutate(title = ifelse(title == "Question and Answers #3 (ft. Caroline Lee)", "Questions and Answers #3 (ft. Caroline Lee)", title),
         ep.num = gsub("[- .)(+!',/]|[a-zA-Z]*:?","", title),
         ep.num = ifelse(title == "2 + 1 - Ep: 73", 73, ep.num),
         ep.num = ifelse(title == "25 Douchebags and a Gentleman - Ep:18", 18, ep.num),
         ep.num = ifelse(title == "Bing Lee and His 500 Teenage Prostitutes - Ep: 4", 4, ep.num),
         ep.num = parse_number(ep.num)
         ) %>%
  filter(!grepl("Questions and Answers", title)) %>%
  arrange(ep.num) 
```


One of the problems with using captions, is the messy text. I used a simple set of `gsub` to transform obvious punctuation marks into their English counterparts. I also pulled out the character SPEAKING the words from the text itself. I left this column alone in the dataset, but might one day go back and focus an analysis on speaking characters.

```{r, message=FALSE}
library(tidytext)
library(stringr)

lizziebennet <- text %>%
  left_join(titles, by="title") %>%
  filter(!is.na(ep.num)) %>%
  arrange(ep.num) %>%
  mutate(linenumber = row_number()) %>%
  mutate(text = gsub("&#39;", "'", text),
         text = gsub("&quot;", '\"', text),
         text = gsub("&amp;", "and", text),
         character = str_extract(text, "^[a-zA-Z]*:"),
         text = sub("^[a-zA-Z]*:", "", text)
         ) %>%
  arrange(ep.num, linenumber)
```


```{r, echo=F}
custom_stop_words <- tibble(
  word = c("margaret", "dunlap", "bernie", "su", "taylor", "brogan", "rachel", "kiley", "kate", "rorick", "jay", "bushman", "alyson", "anne", "toole"), 
  lexicon = "custom")

stop_words <- stop_words %>% 
  bind_rows(custom_stop_words)
```


Okay, so now the text is mostly... in place. The first thing I did was look at word counts. The  most common words are not surprising, it's just a list of the characters.

```{r, message=FALSE, warning=TRUE, collapse=TRUE}
lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words, by="word") %>%
  count(word, sort=TRUE) %>%
  top_n(10)
```

The ngrams are also good to look at. Not surprisingly, the most common tri-gram is from the phrase, "My name is Lizzie Bennet and..."
```{r, message=FALSE}
lizziebennet %>%
  tidytext::unnest_tokens(word, text, token="ngrams", n=3) %>%
  count(word, sort=TRUE) %>%
  top_n(10)
```
I'm especially amused by THE MOST AWKWARD DANCE EVER being in the top 10 5-grams.
```{r, echo=FALSE, message=FALSE}
lizziebennet %>%
  tidytext::unnest_tokens(word, text, token="ngrams", n=5) %>%
  count(word, sort=TRUE) %>%
  ungroup() %>%
  top_n(10)
```


## Sentiment Analysis

I've chosen to use the Bing lexicon (because of Bing Lee, get it?). In Tidydata sentiment analysis is easy because you just join the dictionary against your tokenzied words. 

```{r, message=FALSE}
bing <- sentiments %>%
        filter(lexicon == "bing") %>%
        select(-score)

lbwordcount <- lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(title)
  
lbsentiment <- lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  inner_join(bing) %>% 
  count(title, index=ep.num, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  left_join(lbwordcount) %>%
  mutate(sentiment = positive - negative,
         sentiment = sentiment / n)  
```

Most positive sentiment episodes
```{r, echo=FALSE, message=FALSE}
lbsentiment %>% 
  ungroup() %>%
  arrange(desc(sentiment)) %>%
  select(title, sentiment) %>%
  top_n(5)
```

Most negative sentiment episodes
```{r echo=FALSE, message=FALSE}
lbsentiment %>% 
  ungroup() %>%
  arrange(sentiment) %>%
  select(title, sentiment) %>%
  top_n(-5)
```
```{r, message=FALSE, warning=FALSE, include=FALSE}
x <- lbsentiment %>%
  ungroup() %>%
  arrange(desc(sentiment)) %>%
  select(index, sentiment) %>%
  top_n(10)

y <- lbsentiment %>%
  ungroup() %>%
  arrange(sentiment) %>%
  select(index, sentiment) %>%
  top_n(-10)


plot_text <- bind_rows(x, y) %>%
  select(index, sentiment) %>%
  mutate(index2 = paste("Ep:", index),
         sentiment = ifelse(sentiment >= 0, sentiment + .0045, sentiment - .0045)
  )
```


```{r, message=FALSE}
library(viridis)
theme_set(theme_bw()) # a theme with a white background

ggplot(lbsentiment, aes(x=index, sentiment, fill=as.factor(index))) +
        geom_bar(stat = "identity", show.legend = FALSE) +
        theme_minimal(base_size = 13) +
        geom_text(data=plot_text, aes(x=index, y=sentiment, label=index), size=3.5) + 
        labs(title = "Sentiment in Lizzie Bennet Diaries",
             y = "Sentiment"
             ) +
        scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1) +
        scale_x_discrete(expand=c(0.02,0)) +
        theme(strip.text=element_text(hjust=0)) +
        theme(strip.text=element_text(face = "italic")) +
        theme(axis.title.x=element_blank()) +
        theme(axis.ticks.x=element_blank()) +
        theme(axis.text.x=element_blank())
```

Julia's sentiment analysis of the original text is much more positive than the LBD analysis, with two negative portions relating to Darcy proposing to Elizabeth and Lydia running away with Wickham. I had expected a similar "Wickham" negative spike, and while that section of Wickham related episodes (Ep 84 to Ep 89) is surely negative it's not more negative than some of the introductory episodes. One could argue, that most of the Lydia - Wickham story line happens off screen and in Lydia's blogs. Which could explain that lack of a clear negative spike.


Continuing the analysis, I wanted to look at which words were causing the largest effect on the overall sentiment. 


```{r, message=FALSE}
bing_word_counts <- lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words, by="word") %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

Given that this is a modern adaption, it's interesting that much like the analysis done on the original "miss" is the top contribution to negative sentiment. In the original text I would assume a higher count of "Miss Bennet's" to the modernized version. However, Lizzie does talk about you she'll miss Charlotte, or she misses her home... etc, so it's not too surprising to see if have a considerable contribution here. I did a bit of an investigation into this with bigrams. 

```{r}
lizziebennet %>%
  tidytext::unnest_tokens(bigram, text, token="ngrams", n=2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(word1 %in% "miss") %>%
  mutate(miss_in_name = ifelse(word2 %in% c("bennet", "lu"), "Yes", "No")) %>%
  count(miss_in_name)
```
And oddly enough, the use of the word "miss" is about half and half between "I miss you" and "Miss Bennet" type phrases. Interesting! 

## More with Bigrams
```{r, message=FALSE}
bigrams_separated <- lizziebennet %>%
  tidytext::unnest_tokens(bigram, text, token="ngrams", n=2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word, 
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)

bigrams_separated %>% 
  ungroup() %>%
  top_n(10) 
```
Not surprisingly, the common bigrams are first and last names of characters, but there's also some fun other popular bigrams with "tour leader" and "video blog." I guess *vlog* wasn't super popular to use on it's own yet.



## Network of Words
One of my favorite part of tidytext is the example on making a bigram network. It's just so fun! 

```{r, fig.height=4, fig.width=8, message=FALSE, warning=FALSE}
library(igraph)
library(ggraph)

set.seed(42)

bigrams_separated %>%
  filter(n > 5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme(axis.title.x=element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  theme(axis.text.x=element_blank()) +
  theme(axis.title.y=element_blank()) +
  theme(axis.ticks.y=element_blank()) +
  theme(axis.text.y=element_blank())
```

I especially enjoy the Bennet sister cluster in the left corner.



I leave you with this last picture.



![](https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-9/1908336_10202570761373587_7013966634375610561_n.jpg?oh=1a5119c2ae93bbd9b01060523cc7e43c&oe=59733FEF)
The cast of LBD and me - Vidcon 2014
