---
title: "Text Analysis of The Lizzie Bennet Diaries"
output:
  html_notebook:
  md_document:
    variant: markdown_github
    fig_width: 7
    fig_height: 6
---

![](http://www.pemberleydigital.com/wp-content/uploads/2012/04/LBD-FacebookCover-Emmy.png)
[Image source](http://www.pemberleydigital.com/)

# Inspiration
Inspired by [Julia's Silge's](http://juliasilge.com/) recent talk on [Tidytext](http://tidytextmining.com/), and her blog posts, I decided to try my hand at some text analysis. Julia's example focus on the works of Jane Austen. As Jane Austen has been adapted so many time, I decided to "adapt" Julia's ideas for the modern works of Austen's book - specifically the Lizzie Bennet Diaries.


# The Lizzie Bennet Diaries
The [Lizzie Bennet Diaries](http://www.pemberleydigital.com/the-lizzie-bennet-diaries/) (also known as the LBD) is a modern adaptation of Jane Austen's Pride and Prejudice for YouTube. The series, created by Hank Green (of Vlogbrothers, Scishow, and Crash course fame) and Bernie Su, tells the famous love story of Elizabeth Bennet and Mr. Darcy through vlogs. Altogether, the series filmed more than 100 video episodes with over 9.5 hours of video making it the longest adaaption of Pride and Prejudice to date. The first video premiered on April 9, 2012 which makes this year the **5th Anniversary** of the series! 


The story is told through Lizzie's vlogs, with major events largely happening of screen. The initial main characters are Lizzie, her friend Charlotte and sisters Lydia and Jane. Together they use costume-theater to act out scenes involving other characters. (Lizzie's version of her mother is my personal favorite.) 

Along with the main LBD channel, there are also some supporting channels. These allow others characters to tell parts of the story that Lizzie doesn't take part in. For example, Lydia's Vlogs include the story on how she meets George Wickham and their budding relationship. While not required viewing, these extra videos help round out the experience.


Since the series ended, 2 books have come out from the creators and writers of the original videos: one that follows the videos but adds some more detail to Lizzie's life, and one that focuses on Lydia's story after the series ends. 

![](https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-9/10329014_10202570761053579_3130716185504613915_n.jpg?oh=d6dce9e54a81d5d7864d68dfa2f6269c&oe=596C631F)
The Secret Diary of Lizzie Bennet, signed by most of the cast and writers

In honor of LBD's 5th Anniversary, let's do some LBD text analysis! **Happy Anniversary LBD!**


# Analysis!
The first part of this analysis is grabbing all the text from YouTube. To access the API, I use the [`tuber`](https://soodoku.github.io/tuber/) package for R written by Gaurav Sood.  


```{r, echo=T, eval=FALSE}
library(tuber)
yt_oauth("app_id", "app_password")
```

```{r, include=FALSE}
library(tuber)
yt_oauth("998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com", "MbOSt6cQhhFkwETXKur-L9rN")
```

First I need the channel id to access the channel's playlist of videos. I don't know what the ID is off the top of my head, but luckily I can do a search for `lizziebennet` to find some videos that I know are part of the series.
```{r}
yt_search("lizziebennet")[1:5, ] %>% 
  select(chanelId, channelTitle) 
```

With the channel ID in hand, I now access the channel's resources to find the playlist ID, which I will use to access all the videos in that playlist. `list_channel_resources` for `tuber` creates a list of channel attributes and buried in that list in the playlist ID.
```{r}
# Channel Information
a <- list_channel_resources(filter = c(channel_id="UCXfbQAimgtbk4RAUHtIAUww"), part="contentDetails")

# Uploaded playlists:
playlist_id <- a$items[[1]]$contentDetails$relatedPlaylists$uploads
```

The YouTube API atomically pages videos so the max you get per page is 50. I know I need more than that, so I created a function that I can use recursively to get all the videos. (This code is not a simple way of doing what I need it to, but it works. I would love any comments on how to clean it up.)

```{r, message=FALSE}
library(tidyverse)

# Get videos on the playlist
vids <- get_playlist_items(filter= c(playlist_id=playlist_id), max_results = 50) 
  vid_ids <- map(vids$items, "contentDetails") %>%
  map("videoId")  %>%
  unlist()
vid_info <- tibble(ids = vid_ids, page = vids$nextPageToken)

getVideos <- function(vid_info){
  pageToken <- vid_info$page[length(vid_info$page)]
  vids <- get_playlist_items(filter= c(playlist_id=playlist_id), page_token = pageToken)
  vid_ids <- map(vids$items, "contentDetails") %>%
    map("videoId")  %>%
    unlist()
  nextPageToken = ifelse(!is.null(vids$nextPageToken), vids$nextPageToken, NA)
  vid_info_new <- tibble(ids = vid_ids, page = nextPageToken)
  return(vid_info %>% bind_rows(vid_info_new) )
}

vid_info <- getVideos(vid_info)
vid_info <- getVideos(vid_info)

# check that I have all 112 videos
nrow(vid_info)
```


Now that I have a list of video IDs, I can use `get_captions` to access the text of the videos. I also use `xmlTreeParse` and ``to covert the caption into into an easily accessible lines of text. I put the text, video ID, and video title in a tibble for use in tidydata.

```{r, message=FALSE}
library(XML)

getText <- function(id){
  x <- get_captions(id, lang = "en")
  title <- get_video_details(id)$title
  a <- xmlTreeParse(x)
  text <- a$doc$children$transcript
  text <- xmlToList(text, simplify = TRUE, addAttributes = FALSE) %>% 
    tibble() %>%
    mutate( id = id, title = title)
  return(text) 
}

vid_ids <- vid_info$ids[3:112]
text <- map_df(vid_ids, getText) %>% set_names(c("text", "vid_id", "title"))
```

I don't actually want to refer to each video by it's full title, so I do some data munching to get each episode's number (1-100). Notice, the 10 Q&A videos do not get a episode number assigned to them. For the sake of this analysis, I've decided to only work with the main 100 episodes.

```{r, message=FALSE}
titles <- text %>%
  distinct(title) %>%
  mutate(title = ifelse(title == "Question and Answers #3 (ft. Caroline Lee)", "Questions and Answers #3 (ft. Caroline Lee)", title),
         ep.num = gsub("[- .)(+!',/]|[a-zA-Z]*:?","", title),
         ep.num = ifelse(title == "2 + 1 - Ep: 73", 73, ep.num),
         ep.num = ifelse(title == "25 Douchebags and a Gentleman - Ep:18", 18, ep.num),
         ep.num = ifelse(title == "Bing Lee and His 500 Teenage Prostitutes - Ep: 4", 4, ep.num),
         ep.num = parse_number(ep.num)
         ) %>%
  filter(!grepl("Questions and Answers", title)) %>%
  arrange(ep.num) 
```


Boop
```{r, message=FALSE}
library(tidytext)
library(stringr)


lizziebennet <- text %>%
  left_join(titles, by="title") %>%
  filter(!is.na(ep.num)) %>%
  arrange(ep.num) %>%
  mutate(linenumber = row_number()) %>%
  mutate(text = gsub("&#39;", "'", text),
         text = gsub("&quot;", '\"', text),
         text = gsub("&amp;", "and", text),
         character = str_extract(text, "^[a-zA-Z]*:"),
         text = sub("^[a-zA-Z]*:", "", text)
         ) %>%
  arrange(ep.num, linenumber)
```

```{r, echo=F}
custom_stop_words <- tibble(
  word = c("margaret", "dunlap", "bernie", "su", "taylor", "brogan", "rachel", "kiley", "kate", "rorick", "jay", "bushman", "alyson", "anne", "toole"), 
  lexicon = "custom")

stop_words <- stop_words %>% bind_rows(custom_stop_words)
```


Okay, so now the text is mostly... in place. The first thing I did was look at word counts. The  most common words are not surprising, it's just a list of the characters.

```{r, message=FALSE, warning=TRUE, collapse=TRUE}
lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words, by="word") %>%
  count(word, sort=TRUE) %>%
  top_n(10)
```

The ngrams are also good to look at. Not surprisingly, the most common tri-gram is from the phrase, "My name is Lizzie Bennet and..."
```{r, message=FALSE}
lizziebennet %>%
  tidytext::unnest_tokens(word, text, token="ngrams", n=3) %>%
  count(word, sort=TRUE) %>%
  top_n(10)
```
I'm especailly amused by THE MOST AWKWARD DANCE EVER being in the top 10 5-grams.
```{r, echo=FALSE, message=FALSE}
lizziebennet %>%
  tidytext::unnest_tokens(word, text, token="ngrams", n=5) %>%
  count(word, sort=TRUE) %>%
  top_n(10)
```


## Sentiment Analysis

I've choosen to use the Bing lexicon (because of Bing Lee, get it?). In Tidydata sentiment analysis is easy because you just join the dictionary against your tokenzied words. 

```{r, message=FALSE}
bing <- sentiments %>%
        filter(lexicon == "bing") %>%
        select(-score)

lbwordcount <- lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(title)
  
lbsentiment <- lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  inner_join(bing) %>% 
  count(title, index=ep.num, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  left_join(lbwordcount) %>%
  mutate(sentiment = positive - negative,
         sentiment = sentiment / n)  
```

Most postive sentiment episdoes
```{r, echo=FALSE, message=FALSE}
lbsentiment %>% 
  ungroup() %>%
  arrange(desc(sentiment)) %>%
  select(title, sentiment) %>%
  top_n(5)
```

Most negative sentiment episdoes
```{r echo=FALSE, message=FALSE}
lbsentiment %>% 
  ungroup() %>%
  arrange(sentiment) %>%
  select(title, sentiment) %>%
  top_n(-5)
```


```{r, message=FALSE}
library(viridis)
theme_set(theme_bw()) # a theme with a white background

#plotpoints <- tibble(ep.num = c(), text=c(""))

ggplot(lbsentiment, aes(x=index, sentiment, fill=as.factor(index))) +
        geom_bar(stat = "identity", show.legend = FALSE) +
        theme_minimal(base_size = 13) +
        labs(title = "Sentiment in Lizzie Bennet Diaries",
             y = "Sentiment"
             ) +
        scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1) +
        scale_x_discrete(expand=c(0.02,0)) +
        theme(strip.text=element_text(hjust=0)) +
        theme(strip.text=element_text(face = "italic")) +
        theme(axis.title.x=element_blank()) +
        theme(axis.ticks.x=element_blank()) +
        theme(axis.text.x=element_blank())

```

This plot is really fasinating to me! There's a lot to unpack.

Julia's sentiment analysis of the orginal text is much more positive than the LBD analysis, with two negative portions relating to Darcy proposing to Elizabeth and Lydia running away with Wickham. I had expected a similar "Wickham" spike, and while that sections of episodes is surely "negative" it's not more negative than some of the introdutory episodes.  

More unpacking... 

More unpacking...


Continuing the anaylsis, I wanted to look at which words were causing the largest effect on the overall sentiment. This is easily done in Tidytext format.

```{r, message=FALSE}
bing_word_counts <- lizziebennet %>%
  tidytext::unnest_tokens(word, text) %>%
  anti_join(stop_words, by="word") %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

Given that this is a modern adpation, it's interesting that much like the analysis done on the orginal "miss" is the top contribution to negative sentiment. In the orignal text I would assume a higher count of "Miss Bennet's" to the moderized version. However, Lizzie does talk about you she'll miss Charolte, or she misses her home... etc, so it's not too surprsing to see if have a considerable contribution here.


## Back to NGrams



```{r, message=FALSE}
bigrams_separated <- lizziebennet %>%
  tidytext::unnest_tokens(bigram, text, token="ngrams", n=2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)

bigrams_separated %>% 
  ungroup() %>%
  top_n(10) 
```

```{r, message=FALSE, warning=FALSE}
library(igraph)
library(ggraph)

bigrams_separated %>%
  filter(n > 5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme(axis.title.x=element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  theme(axis.text.x=element_blank()) +
  theme(axis.title.y=element_blank()) +
  theme(axis.ticks.y=element_blank()) +
  theme(axis.text.y=element_blank())
```





![](https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-9/1908336_10202570761373587_7013966634375610561_n.jpg?oh=1a5119c2ae93bbd9b01060523cc7e43c&oe=59733FEF)
The cast of LBD and me - Vidcon 2014
